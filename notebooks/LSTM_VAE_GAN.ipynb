{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "e-zIsFxcXxiv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K, models\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FXBrB8NTYLlU"
      },
      "outputs": [],
      "source": [
        "# === CONFIG ===\n",
        "sequence_length = 12 # 12 hours of data\n",
        "input_csv = r'C:\\Users\\brian\\Documents\\Project\\data\\processed\\ieee13_multitype_fdia.csv'\n",
        "\n",
        "df = pd.read_csv(input_csv)\n",
        "df = df.drop(columns=[\"timestamp\", \"fdia_target_bus\", \"day\"])\n",
        "df = df.sort_values(by=[\"hour\"]).reset_index(drop=True)\n",
        "\n",
        "label_fdia = df[\"fdia\"].values\n",
        "label_fdia_type = df[\"fdia_type\"].values\n",
        "features = df.drop(columns=[\"fdia\", \"fdia_type\"]).values\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "i7RvLU8oYPxh"
      },
      "outputs": [],
      "source": [
        "# === BUILD SEQUENCES ===\n",
        "X, y_fdia, y_fdia_type = [], [], []\n",
        "for i in range(len(features_scaled) - sequence_length):\n",
        "    X_seq = features_scaled[i:i + sequence_length]\n",
        "    y_label_fdia = label_fdia[i + sequence_length - 1]  # Binary FDIA\n",
        "    y_label_type = label_fdia_type[i + sequence_length - 1]  # Multi-class\n",
        "    X.append(X_seq)\n",
        "    y_fdia.append(y_label_fdia)\n",
        "    y_fdia_type.append(y_label_type)\n",
        "\n",
        "X = np.array(X)\n",
        "y_fdia = np.array(y_fdia)\n",
        "y_fdia_type = np.array(y_fdia_type)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_fdia, test_size=0.2, random_state=42)\n",
        "X_train_type, X_test_type, y_train_type, y_test_type = train_test_split(X, y_fdia_type, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "T8O3HjzpYcoB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data prepared and saved.\n",
            "X shape: (156, 12, 18)\n",
            "Train set: (124, 12, 18) (124,)\n",
            "Test set: (32, 12, 18) (32,)\n",
            "TF dataset example: (32, 12, 18)\n"
          ]
        }
      ],
      "source": [
        "# === TF.DATA PIPELINE ===\n",
        "def create_tf_dataset(X, y, batch_size=32, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(X))\n",
        "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = create_tf_dataset(X_train, y_train)\n",
        "test_ds = create_tf_dataset(X_test, y_test, shuffle=False)\n",
        "\n",
        "# === PREVIEW ===\n",
        "print(\"Data prepared and saved.\")\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)\n",
        "print(\"TF dataset example:\", next(iter(train_ds))[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2k8nbEkdYj9A"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# === LSTM Encoder ===\n",
        "def build_encoder(input_dim, timesteps, intermediate_dim, latent_dim):\n",
        "    inputs = tf.keras.Input(shape=(timesteps, input_dim))\n",
        "\n",
        "    # Set initializer for LSTM weights to avoid gradient issues.\n",
        "    kernel_initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
        "    recurrent_initializer = tf.keras.initializers.Orthogonal(seed=42)\n",
        "\n",
        "    x = layers.LSTM(intermediate_dim, kernel_initializer=kernel_initializer,\n",
        "                    recurrent_initializer=recurrent_initializer)(inputs)\n",
        "    \n",
        "    # Mean and Variance in Latent Space\n",
        "    z_mean = layers.Dense(latent_dim, kernel_initializer=kernel_initializer)(x)\n",
        "    z_log_sigma = layers.Dense(latent_dim, kernel_initializer=kernel_initializer)(x)\n",
        "    \n",
        "    # Sampling Function\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_sigma = args\n",
        "        epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0, seed=42)\n",
        "        return z_mean + tf.exp(z_log_sigma / 2) * epsilon\n",
        "\n",
        "    # Wrap Sampling Function in a Lambda Layer\n",
        "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
        "\n",
        "    # Build the Encoder Model\n",
        "    encoder = models.Model(inputs, [z_mean, z_log_sigma, z], name=\"encoder\")\n",
        "    return encoder\n",
        "\n",
        "# === LSTM Decoder ===\n",
        "def build_generator(latent_dim, timesteps, output_dim, intermediate_dim):\n",
        "    inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "    # Set initializer for LSTM weights to avoid gradient issues.\n",
        "    kernel_initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
        "    recurrent_initializer = tf.keras.initializers.Orthogonal(seed=42)\n",
        "\n",
        "    # Adjust the latent input to match LSTM input shape by repeating the latent vector 'timesteps' times.\n",
        "    repeated_z = layers.RepeatVector(timesteps)(inputs)\n",
        "\n",
        "    # Single LSTM layer to generate the sequence, with 'return_sequences=True' to get output at each time step.\n",
        "    x = layers.LSTM(intermediate_dim, return_sequences=True, kernel_initializer=kernel_initializer, recurrent_initializer=recurrent_initializer)(repeated_z)\n",
        "\n",
        "    # Output layer: Generates time series data.\n",
        "    # TimeDistributed is used to ensure that Dense is applied at every time step.\n",
        "    # Sigmoid activation is used to constrain the output between 0 and 1.\n",
        "    outputs = layers.TimeDistributed(layers.Dense(output_dim, activation='sigmoid', kernel_initializer=kernel_initializer))(x)\n",
        "\n",
        "    # Build the generator model.\n",
        "    generator = models.Model(inputs, outputs, name='generator')\n",
        "    return generator\n",
        "\n",
        "# === Discriminator ===\n",
        "def build_discriminator(timesteps, input_dim, intermediate_dim):\n",
        "    inputs = tf.keras.Input(shape=(timesteps, input_dim))\n",
        "    # Set initializer for LSTM weights to avoid gradient issues.\n",
        "    kernel_initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
        "    recurrent_initializer = tf.keras.initializers.Orthogonal(seed=42)\n",
        "\n",
        "    # LSTM layer: Processes the input sequence and returns outputs at each time step.\n",
        "    lstm_out = layers.LSTM(intermediate_dim, return_sequences=True, kernel_initializer=kernel_initializer, recurrent_initializer=recurrent_initializer)(inputs)\n",
        "\n",
        "    # Only the output from the last time step is used for final classification.\n",
        "    # This Dense layer applies a sigmoid activation to return a probability (0 to 1).\n",
        "    final_output = layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer)(lstm_out[:, -1, :])\n",
        "\n",
        "    # Build the discriminator model, which outputs both the final_output and the lstm_out (Calculating the loss function is required).\n",
        "    discriminator = models.Model(inputs, outputs=[final_output, lstm_out], name=\"discriminator\")\n",
        "    return discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cc8r_2DTZZjk"
      },
      "outputs": [],
      "source": [
        "# === PARAMETERS ===\n",
        "# These will be updated based on actual data dimensions in the next cell\n",
        "latent_dim = 10 # Dimensionality of the latent space.\n",
        "# timesteps and input_dim will be determined from data shape\n",
        "intermediate_dim = 60 # Hidden units in the LSTM Layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "E8SJmx2QaLQw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (124, 12, 18)\n",
            "X_test shape: (32, 12, 18)\n",
            "Updated parameters: timesteps=12, input_dim=18\n"
          ]
        }
      ],
      "source": [
        "# Check current shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "# The data already has the correct 3D shape (samples, timesteps, features)\n",
        "# No need to reshape if it's already 3D, just update the parameters\n",
        "if len(X_train.shape) == 3:\n",
        "    # Update parameters to match actual data dimensions\n",
        "    timesteps = X_train.shape[1]  # sequence length (12)\n",
        "    input_dim = X_train.shape[2]  # number of features per timestep\n",
        "    print(f\"Updated parameters: timesteps={timesteps}, input_dim={input_dim}\")\n",
        "else:\n",
        "    # If it's 2D, we need to determine how to reshape\n",
        "    total_features = X_train.shape[1]\n",
        "    sequence_length = 12  # from config\n",
        "    input_dim = total_features // sequence_length\n",
        "    \n",
        "    X_train = X_train.reshape(X_train.shape[0], sequence_length, input_dim)\n",
        "    X_test = X_test.reshape(X_test.shape[0], sequence_length, input_dim)\n",
        "    \n",
        "    timesteps = sequence_length\n",
        "    print(f\"Reshaped data: timesteps={timesteps}, input_dim={input_dim}\")\n",
        "    print(\"New X_train shape:\", X_train.shape)\n",
        "    print(\"New X_test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0G1YwmEwcUdW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def kl_divergence_loss(z_mean, z_log_sigma):\n",
        "    kl_loss = -0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
        "    return kl_loss\n",
        "\n",
        "def reconstruction_loss(discriminator_hidden_output, discriminator_hidden_output_1):\n",
        "    return tf.reduce_mean(tf.square(discriminator_hidden_output - discriminator_hidden_output_1), axis=-1)\n",
        "\n",
        "def encoder_loss(z_mean, z_log_sigma, discriminator_hidden_output, discriminator_hidden_output_1):\n",
        "    kl_loss = kl_divergence_loss(z_mean, z_log_sigma)\n",
        "    re_loss = reconstruction_loss(discriminator_hidden_output, discriminator_hidden_output_1)\n",
        "    encoder_loss = tf.reduce_mean(kl_loss) + tf.reduce_mean(re_loss)\n",
        "    return encoder_loss\n",
        "\n",
        "def discriminator_loss(discriminator_output_1, discriminator_output_2, discriminator_output_real):\n",
        "    discriminator_loss = -K.mean(K.log(1 - discriminator_output_1) + K.log(1 - discriminator_output_2) + K.log(discriminator_output_real))\n",
        "    return discriminator_loss\n",
        "\n",
        "def generator_loss(discriminator_output_1, discriminator_output_2, discriminator_hidden_output, discriminator_hidden_output_1):\n",
        "    adv_loss = -K.mean(K.log(discriminator_output_1 + 1e-10) + K.log(discriminator_output_2 + 1e-10))\n",
        "    re_loss = reconstruction_loss(discriminator_hidden_output, discriminator_hidden_output_1)\n",
        "    generator_loss = adv_loss + tf.reduce_mean(re_loss)\n",
        "    return generator_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "i2T57kkIcsx9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        }
      ],
      "source": [
        "encoder_optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "encoder = build_encoder(input_dim, timesteps, intermediate_dim, latent_dim)\n",
        "generator = build_generator(latent_dim, timesteps, input_dim, intermediate_dim)\n",
        "discriminator = build_discriminator(timesteps, input_dim, intermediate_dim)\n",
        "\n",
        "@tf.function\n",
        "def train_step(x):\n",
        "    # Persistent GradientTape to allow multiple gradient calculations\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Forward pass through the encoder\n",
        "        # Encode the input x and obtain z_mean, z_log_sigma, and the sampled latent vector z\n",
        "        z_mean, z_log_sigma, z = encoder(x, training=True)\n",
        "\n",
        "        # Generate reconstructed data using the sampled latent vector z\n",
        "        reconstructed_x1 = generator(z, training=True)\n",
        "\n",
        "        # Generate a random latent vector z_random and use it to create another generated data\n",
        "        z_random = tf.random.normal(shape=(1, latent_dim), seed=42)\n",
        "        reconstructed_x2 = generator(z_random, training=True)\n",
        "\n",
        "        # Discriminator output for real input data\n",
        "        discriminator_output_real, discriminator_hidden_output = discriminator(x, training=True)\n",
        "\n",
        "        # Discriminator output for data reconstructed from z\n",
        "        discriminator_output_1, discriminator_hidden_output_1 = discriminator(reconstructed_x1, training=True)\n",
        "\n",
        "        # Discriminator output for data generated from random latent vector z_random\n",
        "        discriminator_output_2, discriminator_hidden_output_2 = discriminator(reconstructed_x2, training=True)\n",
        "\n",
        "        # Compute the losses\n",
        "        # Encoder loss combines KL divergence and reconstruction loss\n",
        "        en_loss = encoder_loss(z_mean, z_log_sigma, discriminator_hidden_output, discriminator_hidden_output_1)\n",
        "\n",
        "        # Generator loss combines adversarial loss and reconstruction loss\n",
        "        gen_loss = generator_loss(discriminator_output_1, discriminator_output_2, discriminator_hidden_output, discriminator_hidden_output_1)\n",
        "\n",
        "        # Discriminator loss measures how well it distinguishes between real and generated data\n",
        "        dis_loss = discriminator_loss(discriminator_output_1, discriminator_output_2, discriminator_output_real)\n",
        "\n",
        "        # Backpropagation step\n",
        "        # Calculate gradients for each model's parameters\n",
        "\n",
        "        # Calculate gradients for encoder based on encoder loss\n",
        "        gradients_of_encoder = tape.gradient(en_loss, encoder.trainable_variables)\n",
        "\n",
        "        # Calculate gradients for generator based on generator loss\n",
        "        gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables)\n",
        "\n",
        "        # Calculate gradients for discriminator based on discriminator loss\n",
        "        gradients_of_discriminator = tape.gradient(dis_loss, discriminator.trainable_variables)\n",
        "\n",
        "        # Apply gradients to update the model parameters\n",
        "        # Update the encoder weights\n",
        "        encoder_optimizer.apply_gradients(zip(gradients_of_encoder, encoder.trainable_variables))\n",
        "\n",
        "        # Update the generator weights\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "        # Update the discriminator weights\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "        # Return the losses for monitoring\n",
        "        return en_loss, gen_loss, dis_loss\n",
        "\n",
        "# Training parameters\n",
        "batchsize = 256\n",
        "epochs = 25\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(X_train), batchsize):\n",
        "        # Get a batch of training data\n",
        "        x = X_train[i:i+batchsize]\n",
        "        # Perform a training step and compute the losses\n",
        "        loss_values = train_step(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lijpLka1dUnE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.11910819, 0.11910819, 0.11910819, 0.11514522, 0.11514522,\n",
              "       0.11514522, 0.12122863, 0.12122863, 0.12122863, 0.12354461,\n",
              "       0.12502342, 0.12502342, 0.11167277, 0.11183628, 0.11183628,\n",
              "       0.0997867 , 0.08858377, 0.08858377, 0.09835184, 0.08763827,\n",
              "       0.08763827, 0.08472872, 0.08909802, 0.08909802, 0.09901838,\n",
              "       0.11081186, 0.11081186, 0.09425934, 0.08312711, 0.08312711,\n",
              "       0.07545411, 0.07527212])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The generator takes the latent variables from the encoder output as input\n",
        "encoded_inputs = encoder.input # Get the input of the encoder (i.e., the original input data)\n",
        "encoded_outputs = encoder.output[2] # Get the third output of the encoder (latent vector z)\n",
        "\n",
        "# Pass the latent vector z from the encoder to the generator, producing the generator's output\n",
        "generated_outputs = generator(encoded_outputs) # Use the generator to create the reconstructed data\n",
        "\n",
        "# The input of VAE_GENERATOR_MODEL is the encoder's input, and the output is the generator's output.\n",
        "VAE_GENERATOR_MODEL = models.Model(encoded_inputs, generated_outputs)\n",
        "\n",
        "# Extract the input and the first output from the discriminator\n",
        "discriminator_inputs = discriminator.input\n",
        "\n",
        "# Get the first output of the discriminator, which usually represents the probability that the input is real\n",
        "discriminator_final_output = discriminator.output[0] # Access the first output layer of the discriminator\n",
        "\n",
        "# Construct a discriminator model that only includes the final output.\n",
        "DISCRIMINATOR = models.Model(inputs=discriminator_inputs, outputs=discriminator_final_output)\n",
        "\n",
        "def compute_anomaly_score(X, alpha):\n",
        "    # Use the VAE-GENERATOR model to generate data based on the input X\n",
        "    generated_output = VAE_GENERATOR_MODEL.predict(X)\n",
        "\n",
        "    # Calculate the reconstruction error (how different the input X is from the generated output)\n",
        "    # Reconstruction error is calculated as the mean absolute error between the original input and the generated output\n",
        "    reconstruction_error = np.mean(np.abs(X - generated_output), axis=(1, 2))\n",
        "\n",
        "    # Pass the input data through the simplified discriminator model to get the first output (usually real/fake probability)\n",
        "    discriminator_final_output = DISCRIMINATOR(X) # Get the first output from the discriminator\n",
        "\n",
        "    # Ensure that the discriminator output is a numpy array\n",
        "    if isinstance(discriminator_final_output, tf.Tensor):\n",
        "        discriminator_final_output = discriminator_final_output.numpy().flatten() # Convert to a numpy array and flatten it if needed\n",
        "\n",
        "    # Compute the anomaly score using a weighted combination of reconstruction error and discriminator output\n",
        "    anomaly_score = np.abs((1 - alpha) * reconstruction_error - alpha * discriminator_final_output)\n",
        "\n",
        "    # Return the calculated anomaly score\n",
        "    return anomaly_score\n",
        "\n",
        "def average_scores(anomaly_scores, window_size, step_size, original_length):\n",
        "    num_points = original_length # The number of points in the original time series\n",
        "    avg_scores = np.zeros((num_points,)) # Array to accumulate anomaly scores for each point\n",
        "    counts = np.zeros((num_points,)) # Array to count how many times each point is included in a sub-sequence\n",
        "\n",
        "    # Loop through each sub-sequence's anomaly score\n",
        "    for i, score in enumerate(anomaly_scores):\n",
        "        # Determine the starting and ending indices of the current sub-sequence within the original time series\n",
        "        start_idx = i * step_size\n",
        "        end_idx = start_idx + window_size\n",
        "\n",
        "        # Accumulate the anomaly score for each point in the current sub-sequence\n",
        "        avg_scores[start_idx:end_idx] += score\n",
        "\n",
        "        # Increment the count for each point to track how many times it is covered by sub-sequences\n",
        "        counts[start_idx:end_idx] += 1\n",
        "\n",
        "    # Avoid division by zero in case some points are not covered by any sub-sequence\n",
        "    counts[counts == 0] = 1\n",
        "\n",
        "    # Compute the average anomaly score by dividing the accumulated score by the count for each point\n",
        "    avg_scores /= counts\n",
        "\n",
        "    return avg_scores\n",
        "\n",
        "alpha = 0.3\n",
        "window_size = 10\n",
        "step_size = 3\n",
        "\n",
        "anomaly_scores = compute_anomaly_score(X_test, alpha)\n",
        "avg_scores = average_scores(anomaly_scores, window_size, step_size, len(X_test))\n",
        "avg_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Q-MzNUAdd0Hw"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# X_test_partial = X_test.loc[2700:3300]\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m true_labels_partial \u001b[38;5;241m=\u001b[39m true_labels\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m2700\u001b[39m:\u001b[38;5;241m3300\u001b[39m]\n",
            "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ],
      "source": [
        "# X_test_partial = X_test.loc[2700:3300]\n",
        "\n",
        "# true_labels = X_test['label'].values\n",
        "# true_labels_partial = true_labels.loc[2700:3300]\n",
        "\n",
        "# sub_test = create_subsequences(X_test_partial, window_size, step_size)\n",
        "# sub_test = sub_test.reshape(sub_test.shape[0], sub_test.shape[1], 1)\n",
        "\n",
        "# num_anomalies = (true_labels_partial == 1).sum()\n",
        "# num_anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aaC3XtmgeRRY"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_test_partial' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_threshold, best_f1_score \u001b[38;5;66;03m# Return the best threshold and the corresponding F1 score.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# original_length1 is the length of X_test_partial.\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m original_length1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mX_test_partial\u001b[49m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Compute anomaly scores.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m anomaly_scores1 \u001b[38;5;241m=\u001b[39m compute_anomaly_score(sub_test, alpha)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_test_partial' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Function to select the best threshold:\n",
        "# Takes candidate thresholds, average anomaly scores, and true labels as input, returns the best threshold and corresponding F1 score.\n",
        "def select_threshold(candidate_thresholds, avg_scores1, true_labels_partial):\n",
        "    best_f1_score = -1 # Initialize the best F1 score as -1, for comparison purposes.\n",
        "    best_threshold = None # Initialize the best threshold as None.\n",
        "\n",
        "    # Iterate through each candidate threshold.\n",
        "    for threshold in candidate_thresholds:\n",
        "        # Convert average anomaly scores to predicted labels by comparing them to the threshold\n",
        "        #(greater than threshold = 1, indicating anomaly; less than or equal = 0, indicating normal).\n",
        "        predicted_labels = (avg_scores1 > threshold).astype(int)\n",
        "\n",
        "        # Compute precision, recall, and F1 score for the current threshold.\n",
        "        precision, recall, f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "            true_labels_partial, predicted_labels, average='binary'\n",
        "        )\n",
        "\n",
        "        # If the F1 score for the current threshold is higher than the best F1 score so far, update the best F1 score and threshold.\n",
        "        if f1_score > best_f1_score:\n",
        "            best_f1_score = f1_score\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_threshold, best_f1_score # Return the best threshold and the corresponding F1 score.\n",
        "\n",
        "# original_length1 is the length of X_test_partial.\n",
        "original_length1 = len(X_test)\n",
        "\n",
        "# Compute anomaly scores.\n",
        "anomaly_scores1 = compute_anomaly_score(X_test, alpha)\n",
        "\n",
        "# Average the scores from sliding windows to get the average anomaly score for each time point.\n",
        "avg_scores1 = average_scores(anomaly_scores1, window_size, step_size, original_length1)\n",
        "\n",
        "# Calculate the minimum and maximum average anomaly scores to define the range for candidate thresholds.\n",
        "min_avg_scores1 = np.min(avg_scores1)\n",
        "max_avg_scores1 = np.max(avg_scores1)\n",
        "\n",
        "# Generate 10 candidate thresholds evenly spaced between the minimum and maximum average anomaly scores.\n",
        "num_candidates = 10\n",
        "candidate_thresholds = np.linspace(min_avg_scores1, max_avg_scores1, num_candidates)\n",
        "\n",
        "# Call the select_threshold function to select the best threshold and the corresponding best F1 score.\n",
        "best_threshold, best_f1_score = select_threshold(candidate_thresholds, avg_scores1, true_labels_partial)\n",
        "\n",
        "test_predicted_labels = (avg_scores1 > best_threshold).astype(int)\n",
        "test_precision, test_recall, test_f1_score, _ = metrics.precision_recall_fscore_support(\n",
        "    true_labels, test_predicted_labels, average='binary'\n",
        ")\n",
        "print(\"Best Threshold:\", best_threshold)\n",
        "print(\"Best F1 Score:\", best_f1_score)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "print(\"Test F1 Score:\", test_f1_score)\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "smartgrid",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
